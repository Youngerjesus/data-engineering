# 7주차 Spark 프로그래밍

airflow 에서 특정 execute_date 에서 DAG 를 성공적으로 완료헀다면 특정 명령어 이외에는 재시작할 수 없다. 
- `airflow tasks test` 로는 실행가능. 
- `reset_dag_run=True` 로 DAG 를 만들어서 실행한다면 가능하다.

## Spark 소개 

Hadoop 이 1세대 빅데이터 처리 기술이라면 Spark 는 2세대 빅데이터 기술. 
- Spark 은 Hadoop 과 동일한 문제를 가지고 있지만 프로그래밍을 하기 쉬워짐 

기본적인 개념은 이렇다. 
- **Spark 과 Hadoop 은 굉장히 큰 데이터를 다수의 작은 데이터 블록을 만들어서 분산 병렬 처리를 하자.** 
- Spark 과 Hadoop 은 데이터가 균등과 분할이 잘되야하는데 Skew 가 되는 문제가 있다. 그래서 특정 데이터 블록이 엄청 커지고 해당 블록을 처리하는 서버는 굉장히 느려진다. 

Hadoop 은 MapReduce 기법을 써서 처리를하는 제약조건이 있어서 굉장히 느렸다. 
- Map + Reduce 단계로 이뤄짐. 디스크 기반의 프로그래밍 모델.
- Map 은 데이터를 병렬로 분할하는 단계로 키-값 데이터로 분할된다. Reduce 는 키 별로 그룹화된 데이터를 통합하는 단계. Reduce 에서 최종적인 결과물을 만든다.
  - Map 과 Reduce 단계에서 디스크에 저장됨. 디스크에 저장되고 다시 읽혀지는 것. 
- Map 과 Reduce 로만 처리해야하기 때문에 복잡한 데이터는 처리하기 어려움.
- 그리고 반복적인 처리 작업에는 디스크 기반 저장 시스템의 입출력 때문에 많이 느리다.
  - 반복적인 처리 작업이란 머신러닝 모델에서 동일한 데이터 셋을 여러번 쓰는데 이 경우에 디스크 기반의 프로그래밍 모델을 쓰면 입출력 때문에 매우 느리다. 
  - Apache Spark 는 메모리 기반의 분산 처리 시스템으로 반복적인 처리 작업에 효율적이다.
    - 메모리가 부족하면 디스크를 쓴다.
  - Apache Spark 는 Map Reduce 방식의 문제를 해결함. 디스크 -> 메모리, 100x 더 빠르다.
  - Apache Spark 은 더 복잡한 연산들 지원. 
    - Spark 은 Pandas DataFrame 와 SQL 을 통해서 프로그래밍 작업을 할 수 있다. (거의 SQL 로 한다고함.)
    - Spark 은 Stream 처리, 배치 처리, SQL, 머신 러닝, 그래프 처리를 지원한다. Hadoop 은 MapReduce 를 이용한 배치 처리만 지원함.

- YARN 은 하둡의 다른 이름이다. 하둡의 2.0 3.0 이름임.

- Spark 은 자체 컴퓨팅 프레임워크를 가지고 있지 않음. 하둡 위나, Kubernetes 위에서 동작하게 할 수 있음. (= Resource Manager)
  - 그리고 스토로지 엔진도 가지고 있지 않음. (= HDFS, AWS S3, Google Cloud Storage 등)

![](./image/spark%20architecture.png)
    

## Spark 프로그래밍

파이썬으로 한다면 DataFrame 을 쓸거고, Java 나 Scala 로 한다면 DataSet 을 할거다. 
- 둘 다 구조는 테이블과 동일함. 판다스의 데이터프레임과 동일. 
- 구조화 데이터 조작이라면 SQL 을 쓰기도 한다. 

RDD (Resilient Distributed Dataset) 라는 것도 있다. Low Level API 로 잘 안씀.

### Spark SQL  

Spark SQL 구조화된 데이터를 처리할 때 SQL 만한 것은 없다고 함. 
- HIVE (= MapReduce 로 구현된 SQL 엔진) 보다 100배 빠름. (메모리니까.) 
  - HIVE 도 디스크기반에서 메모리를 쓰는걸로 발전해서 100배는 아님.

### Spark ML 

원스톱 ML 프레임워크이다.
- DataFrame 과 Spark SQL 을 이용해서 데이터를 전처리하고 Spark ML 을 이용해서 모델을 빌딩한다. ML Pipeline 을 통해서 모델 빌딩을 자동화할 수 있다.
- MLFlow 로 모델을 관리하고 서빙하는 것도 가능하다.
- 대용량 데이터 처리도 가능하다.

- (SQL 기반으로 데이터를 처리하는게 많다. 구조화된 데이터 처리에는 이만한게 없어서 그런가. Ksql 도 있고.. SQL 을 확실하게 해야할듯.)
  - SQL 이 좀 더 READABLE 하다. 

### Spark 의 사용 예 

대용량 데이터 배치 처리, 스트림 처리, 모델 빌딩 
- 대용량 비구조화 데이터 처리 (ETL 혹은 ELT)
- ML 모델에 사용하는 대용량 피처 처리 
  - 피처는 머신러닝 모델에 input 으로 들어가는 데이터라고 보면 됨. 여기서 계산을 때려서 주는 것.
- Spark ML 을 이용해서 Train 데이터를 통한 모델 학습

대용량 비구조화 데이터 처리 
- S3 에 있는 비구조화 데이터를 Spark 이 처리해서 데이터 웨어하우스로 적재.
- Log 데이터를 -> S3 에 적재하는 걸 ETL 이라고 보고, S3 (= 데이터 레이크) 에 있는 데이터를 Spark 에서 처리하는 걸 ELT 라고.

### Spark 프로그램 구조 

실행하는 코드의 마스터역할을 하는 Driver 와 실제 테스크를 수행하는 Executor 로 나뉜다.
- Spark Cluster 에 Driver 와 Executor 가 있다. 

Executor 별로 CPU 와 메모리가 할당된다.

### 데이터를 병렬 처리 하려면? 

데이터가 먼저 분산되야한다. 
- 하둡은 기본 적으로 데이터가 분산되는 블락 하나당 128 MB 이다. Spark 에서는 이를 Partitoin 이라고 부른다.
- Spark Executor 에서 병렬로 처리함.
- 적절한 Partition 수 = Executor * Executor CPU Core 수  

원하는 결과까지 데이터 프레임을 계속 변환할 수도 있다. 그리고 데이터 프레임은 기본적으로 Immutable 함.

### 셔플링 

파티션간에 데이터 이동이 불가피하게 발생하는 경우에 생긴다. 
- 셔플링 중에 Data Skew 가 생길 수도 있다. 
  - 이를 최적화하는 방법도 있다.
- 명시적인 파티션의 개수를 줄이려고 할 때 발생할 수 있다.
- 시스템에 의해서 셔플링이 생길 수 있다. (= Sorting, Aggregation (= Group By))
- 셔플링은 네트워크를 통해서 데이터를 전송한다.

### Data Skew 해결법 

- 샘플링 기반의 파티션 키 분포 조정
  - 이터셋의 일부를 샘플링하여 파티션 키의 분포를 확인하고, 이를 기반으로 작업을 분할하는 방법입니다. 샘플링을 통해 키의 분포를 파악하면 불균형한 데이터 분포를 미리 확인하고 적절한 처리를 할 수 있습니다.

- 사용자 정의 파티셔닝 
  - 데이터의 특성을 고려하여 사용자 정의 파티셔닝 함수를 작성할 수 있습니다. 이를 통해 데이터를 더 균등하게 분산시킬 수 있어 데이터 스큐 현상을 완화할 수 있습니다.

- 키 값에 추가적인 정보를 제공해줘서 균등하게 분포되도록 

### 파티션 수 조절

전체 작업 성능을 최적화하기 위해 적절한 파티션 수를 설정하는 것은 중요하다.
- 초기 데이터 로딩 시: 데이터를 처음 로드할 때 적절한 수의 파티션으로 나누어 처리 작업을 병렬화하고 성능을 향상시키려는 경우에 사용할 수 있습니다.
- 조인 연산 수행 시: 두 개의 데이터셋을 조인할 때, 적절한 수의 파티션으로 데이터를 분할하여 조인 작업을 병렬화하고 처리 성능을 향상시키려는 경우에 사용할 수 있습니다.

## Spark 프로그램 구조

Spark 프로그램의 시작은 Session 을 만드는 것으로 시작한다. 
- Spark Cluster 와 통신하는 Singleton 객체
- Spark 프로그램마다 하나씩 만들어야한다. 
- Session 을 통해서 Spark 이 제공해주는 기능을 쓸 수 있음.

Spark 프로세서와 Python Processor 는 별개로 분리된다. 
- collect() 나 show() 함수를 호출하면 Spark 에서 Python 으로 데이터를 가지고온다. 그래서 큰 데이터라면 쓰면 안됨.

![](./image/spark%20session.png)

- 파이썬으로 Spark Session 만드는 법

### Spark 데이터 플로우 

![](./image/spark%20data%20flow%20.png)


## 커리어에서 어떤 점이 중요한가. 

결과를 내는 것. 
- 결과를 잘 내기 위해선 문제 정의가 중요하다. 
- 성공과 실패를 정의한느 것도 중요. 
- 그리고 이를 결정권자들과 잘 이야기하는 게 중요하고. (커뮤니케이션 능력이 중요.)
- 우선순위가 높은 일들을 아는게 중요하다. 
- 자기 검열 안하고, 질문 잘하는게 중요. 
- 완벽하게 하는 것보다 완료하는게 더 나을 수 있다.

## 복리가 있는 일 하기 

Practice Makes Perfect 
- 책 읽기
- 운동 
- 배움/학습 
- 네트워킹  

## 이력서에 적을 때 STAR (Situation, Task, Action, Result) 방식으로 적어야한다.